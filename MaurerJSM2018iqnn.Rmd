---
title: "Iterative Quantiles Nearest-Neighbors"
author: "Karsten Maurer" 
institute: "Miami University"
date: "July 30, 2018"
output:
  beamer_presentation:
    theme: "Madrid"
    colortheme: "beaver"
    toc: false
    slide_level: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

# libraries
library(iqbin)
library(FNN)
library(tidyverse)
library(stringr)
library(randomForest)
library(RANN)
library(mvtnorm)
library(gridExtra)
library(xtable)

# Load all data
load(file="./sim_all.Rdata")
load(file="./sim_times_agg.Rdata")
```

## Introduction

$k$ Nearest-Neighbors (KNN): 

- Finds $k$ most similar points to a query point
- Regression and classification applications
- Computationally expensive distance calculations and sorting

Approximate Nearest Neighbors (AKNN):

- Efficient algorithms for approximate nearest neighborhoods
- kd-tree AKNN (Arya et al., 1998) 
- cover-tree AKNN (Beygelzimer et al., 2006)

Iterative Quantile Nearest-Neighbors (IQNN):

- Can we make neighborhoods with binned-partitions of feature space?
- Partition with $k$ training points per partition
- Use iterative algorithm of quantile-based univariate partitions

## IQNN - Simple Demonstration

```{r p1, fig.align='center',fig.width=4, fig.height=3}
set.seed(12345)
nsim=90
simbins <- 3
color_steps <- c(-4,-2,0,2,4)
mydata <- data.frame(rmvnorm(nsim, mean=c(0,0), sigma=matrix(c(1,.9,.9,1),byrow=TRUE, nrow=2)))
mydata$Y <- mydata$X1 + mydata$X2 + rnorm(nsim)
point_color <- "black"
line_color <- "black"
mytheme <- theme_bw() + 
  theme(legend.position = "right",panel.grid.minor = element_blank(),
        panel.border = element_blank(), legend.text.align = 1, legend.title.align = .25)

mybins <- iqbin(data=mydata, bin_cols=c("X1","X2"),nbins=c(simbins, simbins), output="both")
X1_bounds <- unique(c(mybins$bin_def$bin_bounds[,1],mybins$bin_def$bin_bounds[,2]))

# plot with X1 breaks 
ggplot()+
  geom_point(aes(x=X1,y=X2, color=Y), data=mydata, size=1) +
  scale_color_gradient2("Y",low="#08519c",mid="gray80",high="#a50f15",
                        midpoint=0,breaks=color_steps,
                        limits=c(-max(abs(mydata$Y)),max(abs(mydata$Y))))+
  labs(x=expression(X[1]),y=expression(X[2]))+
  mytheme +
  annotate(geom="text",x=-1.5,y=1.5,label="n=90", size=4.5)
```

## IQNN - Simple Demonstration

```{r p2, fig.align='center',fig.width=4, fig.height=3}
ggplot()+
  geom_point(aes(x=X1,y=X2, color=Y), data=mydata, size=1) +
  geom_vline(xintercept = X1_bounds, size=.6, color=line_color)+
  scale_color_gradient2("Y",low="#08519c",mid="gray80",high="#a50f15",
                        midpoint=0,breaks=color_steps,
                        limits=c(-max(abs(mydata$Y)),max(abs(mydata$Y))))+
  labs(x=expression(X[1]),y=expression(X[2]))+
  mytheme +
  annotate(geom="text",x=-1.5,y=1.5,label="n[b]==30", size=4.5, parse=T)
```

## IQNN - Simple Demonstration

```{r p3, fig.align='center',fig.width=4, fig.height=3}
ggplot()+
  geom_point(aes(x = X1, y = X2,color=Y), data = mydata, size=1)+  
  geom_rect(aes(xmin = X1, xmax = X2, ymin = X3, ymax = X4), 
            data = data.frame(mybins$bin_def$bin_bounds),
            color =line_color, fill = NA, size=.6) +
  scale_color_gradient2("Y",low="#08519c",mid="gray80",high="#a50f15",
                        midpoint=0,breaks=color_steps,
                        limits=c(-max(abs(mydata$Y)),max(abs(mydata$Y))))+
  labs(x=expression(X[1]),y=expression(X[2]))+
  mytheme +
  annotate(geom="text",x=-1.5,y=1.5,label="n[b]==10", size=4.5, parse=T)
```

## IQNN - Simple Demonstration

```{r p4, fig.align='center',fig.width=4, fig.height=3}
# IQNN regression plot
myiqnn <- iqnn(mydata,y="Y", mod_type = "reg", bin_cols=c("X1","X2"),nbins=c(simbins,simbins))
ggplot()+
  geom_rect(aes(xmin = X1, xmax = X2, ymin = X3, ymax = X4, fill=pred), 
            data = data.frame(mybins$bin_def$bin_bounds,pred=myiqnn$bin_stats$pred),
            color =line_color, size=.6) +
  scale_fill_gradient2(expression(hat(Y)),low="#08519c",mid="gray80",high="#a50f15",
                       midpoint=0,breaks=color_steps,
                       limits=c(-max(abs(mydata$Y)),max(abs(mydata$Y))))+
  labs(x=expression(X[1]),y=expression(X[2]))+
  mytheme +
  annotate(geom="text",x=-1.5,y=1.5,label="n[b]==10", size=4.5, parse=T)
```



## IQNN Query Structure
\begin{figure}[hbtp]
\centering
  \includegraphics[width=.99\textwidth]{IntervalRtreeExample.png}
  \caption{Interval R-tree structure generated by iterative quantile binning in simulated feature data example from above.}
\end{figure}

## Evaluating IQNN Performance

Computational Efficiency: \textit{Timing study}

- Test with simulated data sets of varying sizes: n=$2^4$,$2^6$,...,$2^{20}$
- Test with various neighborhood sizes: $k$=$2^0$,$2^4$,...,$2^{14}$
- Speed of pre-processing with IQNN vs AKNN methods
- Speed of identifying neighboring points with IQNN vs AKNN methods

## Timing Study

```{r timing_plot, fig.align='center',fig.width=9.5, fig.height=5.3, out.width='.99\\linewidth', warning=FALSE}
sim_fit_times <- sim_times_agg %>%
  select(-ends_with("predtime")) %>%
  gather(key="type",value="time",knn_brute_fittime:iqnn_fittime) %>%
  mutate(stage = "fitting",
         type = str_sub(type,1,-9),
         plabel = factor(paste0("p == 2^",log2(p)), levels=paste0("p == 2^",sort(unique(log2(sim_times_agg$p))))),
         klabel = factor(paste0("k == 2^",log2(k)), levels=paste0("k == 2^",sort(unique(log2(sim_times_agg$k))))) )

# pull out prediction times and move from wide to tall
sim_pred_times <- sim_times_agg %>%
  select(-ends_with("fittime")) %>%
  gather(key="type",value="time",knn_brute_predtime:iqnn_predtime) %>%
  mutate(stage = "predicting",
         type = str_sub(type,1,-10),
         plabel = factor(paste0("p == 2^",log2(p)), levels=paste0("p == 2^",sort(unique(log2(sim_times_agg$p))))),
         klabel = factor(paste0("k == 2^",log2(k)), levels=paste0("k == 2^",sort(unique(log2(sim_times_agg$k))))),
         dlabel = factor(paste0("delta == 2^",log2(d)), levels=paste0("delta == 2^",sort(unique(log2(sim_times_agg$d))))) )


# define color palette
my_colors <- RColorBrewer::brewer.pal(n=4,name="Set1")[c(1,4,2,3)]

# combine and define transformation of strings to clarify labels
combined_times <- rbind(data.frame(sim_fit_times,dlabel=NA),
                        sim_pred_times)

combined_times$stage_pretty <- factor(ifelse(combined_times$stage=="fitting","Preprocessing","Prediction"),
                                      levels=c("Preprocessing","Prediction"))
combined_times$type_pretty <- factor(combined_times$type, labels=c("IQNN   ","KNN-brute   ","AKNN-cover   ", "AKNN-kd   "))

### lineplot of times with overlayed points (allows for better transition to grayscale)
ggplot()+
  geom_hline(yintercept = 0)+
  # geom_vline(xintercept = 2^14)+
  geom_line(aes(x=n, y=time, color=type_pretty, linetype=type_pretty),
            size=.6,data=combined_times) +
  geom_point(aes(x=n, y=time, color=type_pretty, shape=type_pretty),
             size=2.5,data=combined_times) +
  # geom_text(aes(x=n, y=time, label=dlabel),
  #           data=filter(sim_pred_times, type=="iqnn"), parse=TRUE) +
  facet_grid(stage_pretty~klabel, labeller = label_parsed)+
  scale_y_continuous("Time in log10(mins)", trans="log10") +
  scale_x_continuous(trans="log2", breaks=2^seq(4,20,by=4), 
                     labels=parse(text=paste0("2^",seq(4,20,by=4)))) +
  # scale_color_brewer(palette="Set1")+
  # scale_linetype_manual("Neighborhood Algorithm:  ",values=c("solid","dotted","twodash","dotdash"))+
  scale_linetype_manual("Neighborhood Algorithm:  ",values=c(1,1,1,1))+
  scale_shape_manual("Neighborhood Algorithm:  ", values=c(2,5,0,1))+
  scale_color_manual("Neighborhood Algorithm:  ", values=my_colors)+
  labs(x="Training Size (n)")+
  theme_bw()+
  theme(legend.position = "bottom")
```

## Evaluating IQNN Performance

Predictive Accuracy: \textit{Empirical Comparison}

- Test accuracy on with many common data sets
- 10 regression problems, 10 classification problems
- Data Repos: UCI (\url{archive.ics.uci.edu}) and KEEL (\url{sci2s.ugr.es/keel})
- Accuracy assessed using 10-fold CV with tuned models from each case


## Regression Accuracy
```{r reg_accuracy, fig.align='center',fig.width=9.5, fig.height=5.3, out.width='.99\\linewidth', warning=FALSE}
load(file="./tuned_regression_testing.Rdata") 
 
#Setting order of data sets
iqnn_reg_rmse <- filter(results_reg, type=="iqnn") %>% arrange(desc(rmse))
iqnn_rmse_order <- as.character(iqnn_reg_rmse$data_name)

# clean data for plots of RMSE relative to knn as baseline
#   - separate knn from others for "baseline" layers vs "points" layers in plotting
reg_plot_data_knn <- results_reg %>%
  filter(type == "knn - brute") %>%
  mutate(data_name = factor(data_name,levels=iqnn_rmse_order),
         x=as.numeric(data_name)-.5,
         xend=as.numeric(data_name)+.5)
reg_plot_data_knn$type_pretty <- "KNN-brute"

shiftval=.25
reg_plot_data <- results_reg %>%
  filter(type != "knn - brute") %>%
  mutate(shift = (as.numeric(as.factor(as.character(type)))-2)*shiftval,
         data_name = factor(data_name,levels=iqnn_rmse_order))
reg_plot_data$type_pretty <- factor(reg_plot_data$type, labels=c("IQNN   ","AKNN-cover ", "AKNN-kd   ")) 

#!# data labels match CLASSIFIERS!
x_label_sizes_reg <- as.numeric(sapply(levels(reg_plot_data$data_name), function(x) reg_plot_data$obs[reg_plot_data$data_name==x][1]))
my_x_ticks_reg <- paste0(levels(reg_plot_data$data_name), "\n n=",x_label_sizes_reg)


# plot relative to KNN on raw RMSE scale
#   - use vertical lines to partition between datasets
#   - baseline KNN-brute as flat line segment
#   - consistently offset points for IWNN and AKNN on X, y=accuracy
#   - scale / theme to match previous plots
p3 <- ggplot()+
  geom_vline(xintercept=seq(0.5,10.5,by=1),linetype=2,color="gray25")+
  geom_segment(aes(x=x, xend=xend,y=rmse,yend=rmse, linetype="KNN-brute"),
               color=RColorBrewer::brewer.pal(4,"Set1")[4], size=.8, data=reg_plot_data_knn) +
  geom_point(aes(x=as.numeric(data_name)+shift,y=rmse,color=type_pretty,shape=type_pretty),
             size=3,data=reg_plot_data)+
  theme_bw() +
  scale_linetype_manual("Baseline: ", values=1)+
  scale_x_continuous("", breaks=1:10,
                     labels=my_x_ticks_reg,
                     limits=c(0.5,10.5)) +
  labs(y="Average RMSE\n (units of std dev)")+
  scale_color_manual("Model Type: ", values=RColorBrewer::brewer.pal(4,"Set1")[c(1,2,3)])+
  scale_shape_manual("Model Type: ", values=c(2,0,1))+
  theme(panel.grid.major.x =element_blank(),
        panel.grid.minor.x =element_blank(),
        axis.ticks.x = element_blank(),
        # panel.border = element_blank(),
        legend.position = "none")


# plot relative to KNN as difference in RMSE from KNN-brute
#   - use vertical lines to partition between datasets
#   - baseline at 0 to represent KNN-brute
#   - consistently offset points for IWNN and AKNN on X, y=accuracy
#   - scale / theme to match previous plots
p4 <- ggplot()+
  geom_segment(aes(x=.5, xend=10.5,y=0,yend=0, linetype="KNN-brute"),size=.8, color=RColorBrewer::brewer.pal(4,"Set1")[4])+
  geom_vline(xintercept=seq(0.5,10.5,by=1),linetype=2,color="gray25")+
  geom_point(aes(x=as.numeric(data_name)+shift,y=rmse_diff,color=type_pretty,shape=type_pretty),
             size=3,data=reg_plot_data)+
  theme_bw() +
  scale_x_continuous(" ", breaks=1:10,
                     labels=my_x_ticks_reg,
                     limits=c(0.5,10.5)) +
  labs(y="Difference from KNN-brute\n (units of std dev)")+
  # scale_y_continuous("Relative CV-RMSE \n (Difference from KNN-brute)", breaks=seq(-.01,.04,by=.01),limits=c(-0.01,0.04)) +
  scale_color_manual("Model Type: ", values=RColorBrewer::brewer.pal(4,"Set1")[c(1,2,3)])+
  scale_shape_manual("Model Type: ", values=c(2,0,1))+
  scale_linetype_manual("Baseline: ", values=1)+
  theme(panel.grid.major.x =element_blank(),
        panel.grid.minor.x =element_blank(),
        axis.ticks.x = element_blank(),
        # panel.border = element_blank(),
        legend.position = "bottom") +
  annotate(geom="text",x=0,y=0,label="bold(KNN-brute)", color=RColorBrewer::brewer.pal(4,"Set1")[4], parse=T,vjust=-.2,hjust=0.4)

grid.arrange(p3,p4,nrow=2,heights=c(1,1.2))
```


## Classifier Accuracy
```{r class_accuracy, fig.align='center',fig.width=9.5, fig.height=5.3, out.width='.99\\linewidth', warning=FALSE}
# Load the Rdata file containing the results_class data frame
load(file="./tuned_classification_testing.Rdata") 
 
#Setting order of data sets
iqnn_class_err <- filter(results_class, type=="iqnn") %>% arrange(avg_cv_accuracy)
iqnn_err_order <- as.character(iqnn_class_err$data_name)
 
# clean data for plots of accuracy relative to knn as baseline
#   - separate knn from others for "baseline" layers vs "points" layers in plotting
class_plot_data_knn <- results_class %>%
  filter(type == "knn - brute") %>%
  mutate(data_name = factor(data_name,levels=iqnn_err_order),
         x=as.numeric(data_name)-.5,
         xend=as.numeric(data_name)+.5)
class_plot_data_knn$type_pretty <- "KNN-brute"

shiftval=.25 # control the horizontal offset of points on plot
class_plot_data <- results_class %>%
  filter(type != "knn - brute") %>%
  mutate(data_name = factor(data_name,levels=iqnn_err_order),
         shift = (as.numeric(as.factor(as.character(type)))-2)*shiftval,
         diff_err_perc = -diff_acc*100)
class_plot_data$type_pretty <- factor(class_plot_data$type, labels=c("IQNN   ","AKNN-cover ", "AKNN-kd   ")) 

x_label_sizes <- as.numeric(sapply(levels(class_plot_data$data_name), function(x) class_plot_data$obs[class_plot_data$data_name==x][1]))
my_x_ticks <- paste0(levels(class_plot_data$data_name), "\n n=",x_label_sizes)

# plot relative to KNN
#   - use vertical lines to partition between datasets
#   - baseline KNN-brute as flat line segment
#   - consistently offset points for IWNN and AKNN on X, y=accuracy
#   - scale / theme to match previous plots
p1 <- ggplot()+
  geom_segment(aes(x=x, xend=xend,y=100-avg_cv_accuracy*100,yend=100-avg_cv_accuracy*100, linetype="KNN-brute"),
               color=RColorBrewer::brewer.pal(4,"Set1")[4], size=.8, data=class_plot_data_knn) +
  geom_vline(xintercept=seq(0.5,10.5,by=1),linetype=2,color="gray25")+
  geom_point(aes(x=as.numeric(data_name)+shift,y=100-avg_cv_accuracy*100,color=type_pretty,shape=type_pretty),
             size=3,data=class_plot_data)+
  theme_bw() +
  scale_linetype_manual("Baseline: ", values=1)+
  scale_x_continuous("", breaks=1:10,
                     labels=my_x_ticks,
                     limits=c(0.5,10.5)) +
  scale_y_continuous("Misclassification Rates\n (units of % error)",
                     breaks=seq(0,100,by=10),labels=paste0("  ",seq(0,100,by=10),"% ")) +
  scale_color_manual("Model Type: ", values=RColorBrewer::brewer.pal(4,"Set1")[c(1,2,3)])+
  scale_shape_manual("Model Type: ", values=c(2,0,1))+
  theme(panel.grid.major.x =element_blank(),
        panel.grid.minor.x =element_blank(),
        axis.ticks.x = element_blank(),
        # panel.border = element_blank(),
        legend.position = "none")


# plot relative to KNN
#   - use vertical lines to partition between datasets
#   - baseline at 0 to represent KNN-brute
#   - consistently offset points for IWNN and AKNN on X, y=accuracy
#   - scale / theme to match previous plots
p2 <- ggplot()+
  geom_segment(aes(x=.5, xend=10.5,y=0,yend=0, linetype="KNN-brute"),size=.8, color=RColorBrewer::brewer.pal(4,"Set1")[4])+
  geom_vline(xintercept=seq(0.5,10.5,by=1),linetype=2,color="gray25")+
  geom_point(aes(x=as.numeric(data_name)+shift,y=diff_err_perc,color=type_pretty,shape=type_pretty),
             size=3,data=class_plot_data)+
  theme_bw()+
  scale_linetype_manual("Baseline: ", values=1)+
  scale_x_continuous(" ", breaks=1:10,
                     labels=my_x_ticks,
                     limits=c(0.5,10.5))+
  scale_y_continuous("Difference from KNN-brute\n (units of % error)",
                     breaks=seq(-0.5,1.5,by=.5),labels=c("-0.5%","0.0%","0.5%","1.0%","1.5%"),
                     limits=c(min(-.5,min(class_plot_data$diff_err_perc)),max(1.5,max(class_plot_data$diff_err_perc)))) +
  scale_color_manual("Model Type: ", values=RColorBrewer::brewer.pal(4,"Set1")[c(1,2,3)])+
  scale_shape_manual("Model Type: ", values=c(2,0,1))+
  # annotate(geom="text",x=0,y=0,label="bold(KNN-brute)", color=RColorBrewer::brewer.pal(4,"Set1")[4], parse=T,vjust=-.2,hjust=0.4)+
  theme(panel.grid.major.x =element_blank(),
        panel.grid.minor.x =element_blank(),
        axis.ticks.x = element_blank(),
        # panel.border = element_blank(),
        legend.position = "bottom")

grid.arrange(p1,p2,nrow=2,heights=c(1,1.2))
```

## Results

Timing Study:

- Requires considerable pre-processing similar to other AKNN methods
- R-tree structure depends only on number of bins - not on $n$

Predictive Accuracy:
 
- Slightly weaker predictions than KNN for regression and classification
- Less fine control on tuning parameters - bins vs $k$
 
Applications:

- Advantage for large n, large k cases
- Example - localized ensemble classification weights (Kuncheva, 2004)

##Acknowledgments

Support for this research was received from both the Air Force Research Lab's Summer Faculty Fellowship Program and Miami University Committee for Faculty Research during different phases this project. 

Additional thanks to Dr. Walter Bennette, Dr. Thomas Fisher and Dr. John Bailer for their advice and input throughout the project. 

## Citations

\footnotesize

Arya, S., Mount, D. M., Netanyahu, N. S., Silverman, R., and Wu, A. Y. (1998), \textit{An optimal algorithm} for approximate nearest neighbor searching fixed dimensions, Journal of the ACM (JACM), 45, 891 (923).

Beygelzimer, A., Kakade, S., and Langford, J. (2006), \textit{Cover trees for nearest neighbor}, in Proceedings of the 23rd international conference on Machine learning, ACM, pp. 97 (104).

Beygelzimer, A., Kakadet, S., Langford, J., Arya, S., Mount, D., and Li, S. (2013), \textit{FNN: Fast Nearest Neighbor Search Algorithms and Applications}, r package version 1.1.

Cover, T. and Hart, P. (1967), \textit{Nearest neighbor pattern classification}, IEEE transactions on information theory, 13, 21 (27).

Fix, E. and Hodges, J. L. (1951), \textit{Discriminatory analysis-nonparametric discrimination: consistency properties}, Tech. rep., California Univ Berkeley.

Kuncheva, L. I. (2004), \textit{Combining pattern classifiers: methods and algorithms}, John Wiley & Sons.

R Core Team (2013), R: A Language and Environment for Statistical Computing, R Foundation for Statistical Computing, Vienna, Austria, http://www.R-project.org/.


## Data Sources

\renewcommand{\arraystretch}{1}
\begin{table}[h]
\tiny
\centering
\begin{tabular}{ll}
\textbf{Classification} & \\
\hline 
Name & Web Address (Accessed: 10/25/17) \\ 
\hline 
  iris & \url{http://archive.ics.uci.edu/ml/machine-learning-databases/iris} \\ 
  wpbc & \url{http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin} \\ 
  pima & \url{http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes} \\
  yeast & \url{http://archive.ics.uci.edu/ml/machine-learning-databases/yeast} \\ 
  abalone & \url{http://archive.ics.uci.edu/ml/machine-learning-databases/abalone} \\ 
  waveform & \url{http://archive.ics.uci.edu/ml/machine-learning-databases/waveform} \\ 
  optdigits & \url{http://sci2s.ugr.es/keel/dataset.php?cod=199} \\ 
  satimage & \url{http://sci2s.ugr.es/keel/dataset.php?cod=71} \\ 
  marketing & \url{http://sci2s.ugr.es/keel/dataset.php?cod=163} \\
  seizure & \url{http://archive.ics.uci.edu/ml/machine-learning-databases/00388} \\ 
\hline 
 & \\
\textbf{Regression} & \\ 
\hline 
Name & Web Address (Accessed: 10/25/17) \\ 
\hline 
  wpbc & \url{http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin} \\ 
  wankara &  \url{http://sci2s.ugr.es/keel/dataset.php?cod=41} \\ 
  laser &  \url{http://sci2s.ugr.es/keel/dataset.php?cod=47} \\ 
  treasury &  \url{http://sci2s.ugr.es/keel/dataset.php?cod=42} \\
  quake & \url{http://sci2s.ugr.es/keel/dataset.php?cod=75} \\ 
  skillcraft &  \url{http://archive.ics.uci.edu/ml/machine-learning-databases/00272} \\
  anacalt & \url{http://sci2s.ugr.es/keel/dataset.php?cod=159} \\
  puma & \url{http://sci2s.ugr.es/keel/dataset.php?cod=1291} \\
  air quality & \url{http://archive.ics.uci.edu/ml/machine-learning-databases/00360} \\
  ccpp & \url{http://archive.ics.uci.edu/ml/machine-learning-databases/00294} \\
   \hline
\end{tabular}
\end{table}
\normalsize

## Thanks!

\begin{center}
Thank you for listening!

\vspace{.2cm}

Any Questions?

\vspace{.2cm}

contact: maurerkt@miamioh.edu
\end{center}

## Algorithm (detail)
\footnotesize 

\noindent \textbf{Specification:} Define order of features $\{X_1,X_2,...,X_p\}$ to match desired iterative binning order and number of bins $\{\delta_1,\delta_2,...,\delta_p\}$ for partitioning in each dimension 

\noindent \textbf{Binning:}
\vspace{-.3cm}
  \begin{enumerate}
  \item Partition all points into $\delta_1$ quantile bins on feature $X_1$ with index sets\\
  $\{B_1,...B_{\delta_1}\}$ such that $B_\ell = \{i \hspace{.2cm} | \hspace{.2cm} b_{X_1}^q(x_{i1}) = \ell \} \hspace{.2cm} \forall \hspace{.2cm} \ell = 1,...,\delta_1$
  \item Repeat the following for $j = 2,...,p$ :
  \vspace{-.3cm}
    \begin{enumerate}[i]
    \footnotesize
    \item Define $C_{s t} = \{i \hspace{.2cm} | \hspace{.2cm} i \in B_s$ and $b_{X_{j}}^q(x_{ij}) = t \} \hspace{.2cm} \forall \hspace{.2cm} s = 1,...,\displaystyle\prod_{d=1}^{j-1}\delta_d$ and $t = 1,...,\delta_j$ \\
    to subdivide each $B_s$ from previous step with $\delta_j$ quantile bins on feature $X_j$
    
    \vspace{.2cm}
    \item Redefine index sets $\{B_1,...,B_L\}$ such that $B_\ell = C_{s t}$, where $\ell = t(s-1) + t$ \\
    to combine parent and child indices of sets into unique indices
    \end{enumerate}
\end{enumerate}

\vspace{-.2cm}
\noindent \textbf{Outputs:}
\vspace{-.2cm}
  \begin{enumerate}[i]
  \vspace{-.4cm}
  \item Bin neighbor sets $\vec{\mathbf{x}}_\ell = \{\vec{x_i} \hspace{.1cm} | \hspace{.1cm}  i \in B_\ell \} \hspace{.2cm} \forall \hspace{.2cm} \ell = 1,...,L$, where $L=\displaystyle\prod_{j=1}^{p}\delta_j$
  \vspace{-.1cm}
    \item Hyper-rectangular bins $\ell = 1,...,L$ containing points $x_{ij} \in (\beta_{j\ell1} \hspace{.1cm} , \hspace{.1cm} \beta_{j\ell2}] \hspace{.1cm} \forall \hspace{.1cm}  j=1,...,p$ 
    \end{enumerate}



