---
title: "Iterative Quantiles Nearest-Neighbors"
author: "Karsten Maurer" 
institute: "Miami University"
date: "July 30, 2018"
output:
  beamer_presentation:
    theme: "Madrid"
    colortheme: "beaver"
    toc: true
    slide_level: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

# libraries
library(iqbin)
library(FNN)
library(tidyverse)
library(stringr)
library(randomForest)
library(RANN)
library(mvtnorm)
library(gridExtra)
library(xtable)
```

# Introduction
## Motivation

$k$ Nearest-Neighbors (KNN): 

- Provides localized non-parametric estimation over feature space
- Computationally expensive distance calculations and sorting
- Efficient algorithms for approximate nearest neightborhoods (AKNN)
- kd-tree AKNN (Arya et al., 1998) 
- cover-tree AKNN (Beygelzimer et al., 2006)

\vspace{.25cm}

Iterative Quantile Nearest-Neighbors (IQNN):

- Can we make neighborhoods with binned-partitions of feature space?
- Checking for points in intervals fast 
- Partition with $k$ training points per partition
- Use iterative algorithm of quantile-based univariate partitions

## IQNN - Simple Demonstration

```{r p1, fig.align='center',fig.width=4, fig.height=3}
set.seed(12345)
nsim=90
simbins <- 3
color_steps <- c(-4,-2,0,2,4)
mydata <- data.frame(rmvnorm(nsim, mean=c(0,0), sigma=matrix(c(1,.9,.9,1),byrow=TRUE, nrow=2)))
mydata$Y <- mydata$X1 + mydata$X2 + rnorm(nsim)
point_color <- "black"
line_color <- "black"
mytheme <- theme_bw() + 
  theme(legend.position = "right",panel.grid.minor = element_blank(),
        panel.border = element_blank(), legend.text.align = 1, legend.title.align = .25)

mybins <- iqbin(data=mydata, bin_cols=c("X1","X2"),nbins=c(simbins, simbins), output="both")
X1_bounds <- unique(c(mybins$bin_def$bin_bounds[,1],mybins$bin_def$bin_bounds[,2]))

# plot with X1 breaks
ggplot()+
  geom_point(aes(x=X1,y=X2, color=Y), data=mydata, size=1) +
  scale_color_gradient2("Y",low="#08519c",mid="gray80",high="#a50f15",
                        midpoint=0,breaks=color_steps,
                        limits=c(-max(abs(mydata$Y)),max(abs(mydata$Y))))+
  labs(x=expression(X[1]),y=expression(X[2]))+
  mytheme +
  annotate(geom="text",x=-1.5,y=1.5,label="n=90", size=5)
```

## IQNN - Simple Demonstration

```{r p2, fig.align='center',fig.width=4, fig.height=3}
ggplot()+
  geom_point(aes(x=X1,y=X2, color=Y), data=mydata, size=1) +
  geom_vline(xintercept = X1_bounds, size=.6, color=line_color)+
  scale_color_gradient2("Y",low="#08519c",mid="gray80",high="#a50f15",
                        midpoint=0,breaks=color_steps,
                        limits=c(-max(abs(mydata$Y)),max(abs(mydata$Y))))+
  labs(x=expression(X[1]),y=expression(X[2]))+
  mytheme +
  annotate(geom="text",x=-1.5,y=1.5,label="n[b]==30", size=5, parse=T)
```

## IQNN - Simple Demonstration

```{r p3, fig.align='center',fig.width=4, fig.height=3}
ggplot()+
  geom_point(aes(x = X1, y = X2,color=Y), data = mydata, size=1)+  
  geom_rect(aes(xmin = X1, xmax = X2, ymin = X3, ymax = X4), 
            data = data.frame(mybins$bin_def$bin_bounds),
            color =line_color, fill = NA, size=.6) +
  scale_color_gradient2("Y",low="#08519c",mid="gray80",high="#a50f15",
                        midpoint=0,breaks=color_steps,
                        limits=c(-max(abs(mydata$Y)),max(abs(mydata$Y))))+
  labs(x=expression(X[1]),y=expression(X[2]))+
  mytheme +
  annotate(geom="text",x=-1.5,y=1.5,label="n[b]==10", size=5, parse=T)
```

## IQNN - Simple Demonstration

```{r p4, fig.align='center',fig.width=4, fig.height=3}
# IQNN regression plot
myiqnn <- iqnn(mydata,y="Y", mod_type = "reg", bin_cols=c("X1","X2"),nbins=c(simbins,simbins))
ggplot()+
  geom_rect(aes(xmin = X1, xmax = X2, ymin = X3, ymax = X4, fill=pred), 
            data = data.frame(mybins$bin_def$bin_bounds,pred=myiqnn$bin_stats$pred),
            color =line_color, size=.6) +
  scale_fill_gradient2(expression(hat(Y)),low="#08519c",mid="gray80",high="#a50f15",
                       midpoint=0,breaks=color_steps,
                       limits=c(-max(abs(mydata$Y)),max(abs(mydata$Y))))+
  labs(x=expression(X[1]),y=expression(X[2]))+
  mytheme +
  annotate(geom="text",x=-1.5,y=1.5,label="n[b]==10", size=5, parse=T)
```


# IQNN Algorithm
## Algorithm
\footnotesize 

\noindent \textbf{Specification:} Define order of features $\{X_1,X_2,...,X_p\}$ to match desired iterative binning order and number of bins $\{\delta_1,\delta_2,...,\delta_p\}$ for partitioning in each dimension 

\noindent \textbf{Binning:}
\vspace{-.3cm}
  \begin{enumerate}
  \item Partition all points into $\delta_1$ quantile bins on feature $X_1$ with index sets\\
  $\{B_1,...B_{\delta_1}\}$ such that $B_\ell = \{i \hspace{.2cm} | \hspace{.2cm} b_{X_1}^q(x_{i1}) = \ell \} \hspace{.2cm} \forall \hspace{.2cm} \ell = 1,...,\delta_1$
  \item Repeat the following for $j = 2,...,p$ :
  \vspace{-.3cm}
    \begin{enumerate}[i]
    \footnotesize
    \item Define $C_{s t} = \{i \hspace{.2cm} | \hspace{.2cm} i \in B_s$ and $b_{X_{j}}^q(x_{ij}) = t \} \hspace{.2cm} \forall \hspace{.2cm} s = 1,...,\displaystyle\prod_{d=1}^{j-1}\delta_d$ and $t = 1,...,\delta_j$ \\
    to subdivide each $B_s$ from previous step with $\delta_j$ quantile bins on feature $X_j$
    
    \vspace{.2cm}
    \item Redefine index sets $\{B_1,...,B_L\}$ such that $B_\ell = C_{s t}$, where $\ell = t(s-1) + t$ \\
    to combine parent and child indices of sets into unique indices
    \end{enumerate}
\end{enumerate}

\vspace{-.2cm}
\noindent \textbf{Outputs:}
\vspace{-.2cm}
  \begin{enumerate}[i]
  \vspace{-.4cm}
  \item Bin neighbor sets $\vec{\mathbf{x}}_\ell = \{\vec{x_i} \hspace{.1cm} | \hspace{.1cm}  i \in B_\ell \} \hspace{.2cm} \forall \hspace{.2cm} \ell = 1,...,L$, where $L=\displaystyle\prod_{j=1}^{p}\delta_j$
  \vspace{-.1cm}
    \item Hyper-rectangular bins $\ell = 1,...,L$ containing points $x_{ij} \in (\beta_{j\ell1} \hspace{.1cm} , \hspace{.1cm} \beta_{j\ell2}] \hspace{.1cm} \forall \hspace{.1cm}  j=1,...,p$ 
    \end{enumerate}

## IQNN Query Structure
\begin{figure}[hbtp]
\centering
  \includegraphics[width=.99\textwidth]{IntervalRtreeExample.png}
  \caption{Interval R-tree structure generated by iterative quantile binning in simulated feature data example from above.}
\end{figure}

# Evaluation
## Evaluating IQNN Performance

Computational Efficiency: \textit{Timing study}

- Test with simulated data sets of varying sizes: n=$2^4$,$2^6$,...,$2^{20}$
- test with various neighborhood sizes: $k$=$2^0$,$2^4$,...,$2^{14}$
- Speed of pre-processing with IQNN vs AKNN methods
- Speed of identifying neighboring points with IQNN vs AKNN methods


Predictive Accuracy: \textit{Timing study}

- Test with real data sets: 10 regression problems, 10 classification problems
- Data Repos: UCI (\url{archive.ics.uci.edu}) and KEEL (\url{sci2s.ugr.es/keel})
- test with various neighborhood sizes: $k$=$2^0$,$2^4$,...,$2^{14}$
- Speed of pre-processing with IQNN vs AKNN methods
- Speed of identifying neighboring points with IQNN vs AKNN methods

